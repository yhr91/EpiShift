{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bedfile_tensor(f):\n",
    "    return Tensor(pd.read_csv(f, index_col=0)['value_scaled'].values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up data split\n",
    "\n",
    "#### NOTE: This has to be corrected so that same patients don't end up on both train and test side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, test = train_test_split(DNAse_labels, random_state=100)\n",
    "#train, val = train_test_split(train, random_state=100, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('./experiment_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "read_in_files = []\n",
    "file_labels = labels['file_accession']\n",
    "for f in file_labels:\n",
    "    print(f)\n",
    "    try:\n",
    "        read_in_files.append(read_bedfile_tensor('./bed_files/'+f+'.bed'))\n",
    "    except:\n",
    "        print('Not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNAse_labels = labels[labels['modality'] == 'DNase-seq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids = train_test_split(DNAse_labels['subject_id'], random_state=100)\n",
    "train_ids, val_ids = train_test_split(train_ids, random_state=100, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = DNAse_labels[DNAse_labels['subject_id'].isin(train_ids)]\n",
    "val_df = DNAse_labels[DNAse_labels['subject_id'].isin(val_ids)]\n",
    "test_df = DNAse_labels[DNAse_labels['subject_id'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "control    0.41875\n",
       "AD         0.31875\n",
       "MCI        0.23750\n",
       "CI         0.02500\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()/len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "control    0.419753\n",
       "AD         0.333333\n",
       "MCI        0.148148\n",
       "CI         0.098765\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['label'].value_counts()/len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_in_tensors = torch.stack([f for f in read_in_files if len(f)==2875012])\n",
    "file_labels = [f for r, f in zip(read_in_files, file_labels) if len(r)==2875012]\n",
    "\n",
    "\n",
    "binary_y = Tensor([t=='AD' for t in labels['label']]).float()\n",
    "binary_labels = torch.stack([f for r, f in zip(read_in_files, binary_y) if len(r)==2875012])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a simple classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up convolutional model without feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpiShift(nn.Module):\n",
    "    \"\"\"\n",
    "    2 1D convolutions followed by a max pool\n",
    "    \n",
    "    Args:\n",
    "        conv_stride:\n",
    "        pool_stride:\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, kernel_size=1000, conv_stride=5, \n",
    "                 pool_stride=5, dilation=2, embed_size=4302):\n",
    "        super(EpiShift, self).__init__()\n",
    "        \n",
    "        self.CNN = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=1, stride=conv_stride, \n",
    "                      kernel_size=kernel_size, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=1, out_channels=1, stride=conv_stride,\n",
    "                      kernel_size=kernel_size, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.Conv1d(in_channels=1, out_channels=1, stride=conv_stride,\n",
    "                      kernel_size=kernel_size, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.MaxPool1d(kernel_size=kernel_size, stride=pool_stride, padding=0, \n",
    "                         dilation=1)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(embed_size, 1)\n",
    "        \n",
    "    def forward(self, data: Tensor):\n",
    "        \n",
    "        embed = self.CNN(data)\n",
    "        x = self.fc1(embed)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# Just logistic regression\n",
    "class LogisticReg(nn.Module):\n",
    "    \"\"\"\n",
    "    2 1D convolutions followed by a max pool\n",
    "    \n",
    "    Args:\n",
    "        conv_stride:\n",
    "        pool_stride:\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, input_size):\n",
    "        super(LogisticReg, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 1)\n",
    "        \n",
    "    def forward(self, data: Tensor):\n",
    "        \n",
    "        x = self.fc1(data)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "# Autoencoder block  \n",
    "def full_block(in_features, out_features, p_drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_features, out_features, bias=True),\n",
    "            nn.LayerNorm(out_features),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=p_drop),\n",
    "        )\n",
    "\n",
    "class FullNet(nn.Module):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, x_dim, hid_dim=128, z_dim=64, p_drop=0):\n",
    "        super(FullNet, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            full_block(x_dim, hid_dim, p_drop),\n",
    "            full_block(hid_dim, z_dim, p_drop),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            full_block(z_dim, hid_dim, p_drop),\n",
    "            full_block(hid_dim, x_dim, p_drop),\n",
    "        )\n",
    "      \n",
    "    def forward(self, x):\n",
    "        \n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        return encoded, decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EpiShift trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DNAse_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, track, labels):\n",
    "        'Initialization'\n",
    "        self.tracks = track\n",
    "        self.labels = labels\n",
    "            \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        return self.tracks[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = np.where([f in train_df['file_accession'].values for f in file_labels])[0]\n",
    "val_idx = np.where([f in val_df['file_accession'].values for f in file_labels])[0]\n",
    "test_idx = np.where([f in test_df['file_accession'].values for f in file_labels])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "train_dataset = DNAse_Dataset(read_in_tensors[train_idx], binary_labels[train_idx])\n",
    "val_dataset = DNAse_Dataset(read_in_tensors[val_idx], binary_labels[val_idx])\n",
    "test_dataset = DNAse_Dataset(read_in_tensors[test_idx], binary_labels[test_idx])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=20)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=20)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Training Loss: 0.83696586\n",
      "Training Loss: 0.8512673\n",
      "Training Loss: 1.3063523\n",
      "Training Loss: 0.82901853\n",
      "Training Loss: 1.2802312\n",
      "Training Loss: 1.4568272\n",
      "Training Loss: 1.1480544\n",
      "Validation Loss: 1.0571274\n",
      "1\n",
      "Training Loss: 0.98301995\n",
      "Training Loss: 0.70448595\n",
      "Training Loss: 0.75097555\n",
      "Training Loss: 1.0437144\n",
      "Training Loss: 1.14103\n",
      "Training Loss: 0.97941\n",
      "Training Loss: 0.8278078\n",
      "Validation Loss: 1.617341\n",
      "2\n",
      "Training Loss: 0.6914822\n",
      "Training Loss: 0.654645\n",
      "Training Loss: 0.6761734\n",
      "Training Loss: 0.55688155\n",
      "Training Loss: 0.60046774\n",
      "Training Loss: 0.9573183\n",
      "Training Loss: 1.0409454\n",
      "Validation Loss: 0.9265917\n",
      "3\n",
      "Training Loss: 0.919652\n",
      "Training Loss: 0.7071683\n",
      "Training Loss: 0.6904067\n",
      "Training Loss: 0.61311495\n",
      "Training Loss: 0.6246263\n",
      "Training Loss: 0.70438874\n",
      "Training Loss: 0.7101474\n",
      "Validation Loss: 1.5077192\n",
      "4\n",
      "Training Loss: 0.6699446\n",
      "Training Loss: 0.6654951\n",
      "Training Loss: 0.6942784\n",
      "Training Loss: 0.70491755\n",
      "Training Loss: 0.6030583\n",
      "Training Loss: 0.74071276\n",
      "Training Loss: 0.79383296\n",
      "Validation Loss: 1.0445639\n",
      "5\n",
      "Training Loss: 0.7129974\n",
      "Training Loss: 0.6364816\n",
      "Training Loss: 0.6595763\n",
      "Training Loss: 0.61563\n",
      "Training Loss: 0.57192427\n",
      "Training Loss: 0.7193361\n",
      "Training Loss: 0.7044528\n",
      "Validation Loss: 1.2586613\n",
      "6\n",
      "Training Loss: 0.6391477\n",
      "Training Loss: 0.6297061\n",
      "Training Loss: 0.6440179\n",
      "Training Loss: 0.6870642\n",
      "Training Loss: 0.5774472\n",
      "Training Loss: 0.72759175\n",
      "Training Loss: 0.7213467\n",
      "Validation Loss: 1.154323\n",
      "7\n",
      "Training Loss: 0.6422174\n",
      "Training Loss: 0.61787015\n",
      "Training Loss: 0.6243548\n",
      "Training Loss: 0.6534139\n",
      "Training Loss: 0.5404602\n",
      "Training Loss: 0.747026\n",
      "Training Loss: 0.6969544\n",
      "Validation Loss: 1.2342179\n",
      "8\n",
      "Training Loss: 0.62939113\n",
      "Training Loss: 0.6108795\n",
      "Training Loss: 0.6006981\n",
      "Training Loss: 0.68886226\n",
      "Training Loss: 0.5266124\n",
      "Training Loss: 0.76739115\n",
      "Training Loss: 0.7047807\n",
      "Validation Loss: 1.227565\n",
      "9\n",
      "Training Loss: 0.62634367\n",
      "Training Loss: 0.61412615\n",
      "Training Loss: 0.5986109\n",
      "Training Loss: 0.6844584\n",
      "Training Loss: 0.53455037\n",
      "Training Loss: 0.7403459\n",
      "Training Loss: 0.6909112\n",
      "Validation Loss: 1.198107\n",
      "10\n",
      "Training Loss: 0.62280124\n",
      "Training Loss: 0.60725874\n",
      "Training Loss: 0.5748235\n",
      "Training Loss: 0.6633224\n",
      "Training Loss: 0.50055355\n",
      "Training Loss: 0.79307795\n",
      "Training Loss: 0.688751\n",
      "Validation Loss: 1.3076117\n",
      "11\n",
      "Training Loss: 0.6310346\n",
      "Training Loss: 0.61369777\n",
      "Training Loss: 0.59464055\n",
      "Training Loss: 0.7393326\n",
      "Training Loss: 0.5391645\n",
      "Training Loss: 0.74056333\n",
      "Training Loss: 0.69989586\n",
      "Validation Loss: 1.1486157\n",
      "12\n",
      "Training Loss: 0.6194486\n",
      "Training Loss: 0.6046192\n",
      "Training Loss: 0.58577865\n",
      "Training Loss: 0.64520544\n",
      "Training Loss: 0.4941678\n",
      "Training Loss: 0.7720588\n",
      "Training Loss: 0.6783409\n",
      "Validation Loss: 1.3174554\n",
      "13\n",
      "Training Loss: 0.6355325\n",
      "Training Loss: 0.6378147\n",
      "Training Loss: 0.57086724\n",
      "Training Loss: 0.6996047\n",
      "Training Loss: 0.53227395\n",
      "Training Loss: 0.75461525\n",
      "Training Loss: 0.7191562\n",
      "Validation Loss: 1.2002957\n",
      "14\n",
      "Training Loss: 0.60991436\n",
      "Training Loss: 0.5866969\n",
      "Training Loss: 0.5552284\n",
      "Training Loss: 0.6545093\n",
      "Training Loss: 0.49125943\n",
      "Training Loss: 0.74900424\n",
      "Training Loss: 0.6527928\n",
      "Validation Loss: 1.3067567\n",
      "15\n",
      "Training Loss: 0.62701374\n",
      "Training Loss: 0.601791\n",
      "Training Loss: 0.546641\n",
      "Training Loss: 0.6570615\n",
      "Training Loss: 0.5065891\n",
      "Training Loss: 0.84658474\n",
      "Training Loss: 0.6238324\n",
      "Validation Loss: 1.4174212\n",
      "16\n",
      "Training Loss: 0.6474267\n",
      "Training Loss: 0.65167844\n",
      "Training Loss: 0.6460509\n",
      "Training Loss: 0.7787369\n",
      "Training Loss: 0.6139395\n",
      "Training Loss: 0.6941228\n",
      "Training Loss: 0.704302\n",
      "Validation Loss: 1.0988821\n",
      "17\n",
      "Training Loss: 0.6241517\n",
      "Training Loss: 0.69087964\n",
      "Training Loss: 0.6584542\n",
      "Training Loss: 0.62773335\n",
      "Training Loss: 0.5527239\n",
      "Training Loss: 0.70470715\n",
      "Training Loss: 0.66516954\n",
      "Validation Loss: 1.3813748\n",
      "18\n",
      "Training Loss: 0.6321326\n",
      "Training Loss: 0.5979127\n",
      "Training Loss: 0.5994029\n",
      "Training Loss: 0.6771464\n",
      "Training Loss: 0.5312822\n",
      "Training Loss: 0.7693391\n",
      "Training Loss: 0.73968405\n",
      "Validation Loss: 1.1666329\n",
      "19\n",
      "Training Loss: 0.60007524\n",
      "Training Loss: 0.5953471\n",
      "Training Loss: 0.58297276\n",
      "Training Loss: 0.6571661\n",
      "Training Loss: 0.528922\n",
      "Training Loss: 0.7092559\n",
      "Training Loss: 0.6470235\n",
      "Validation Loss: 1.2929392\n",
      "20\n",
      "Training Loss: 0.62200516\n",
      "Training Loss: 0.597472\n",
      "Training Loss: 0.5565959\n",
      "Training Loss: 0.67287225\n",
      "Training Loss: 0.50573176\n",
      "Training Loss: 0.746498\n",
      "Training Loss: 0.689854\n",
      "Validation Loss: 1.2545795\n",
      "21\n",
      "Training Loss: 0.61334556\n",
      "Training Loss: 0.5854459\n",
      "Training Loss: 0.53770965\n",
      "Training Loss: 0.6627025\n",
      "Training Loss: 0.50232726\n",
      "Training Loss: 0.7372925\n",
      "Training Loss: 0.6633325\n",
      "Validation Loss: 1.3273284\n",
      "22\n",
      "Training Loss: 0.6265108\n",
      "Training Loss: 0.5897782\n",
      "Training Loss: 0.54483634\n",
      "Training Loss: 0.67997444\n",
      "Training Loss: 0.49671322\n",
      "Training Loss: 0.7327612\n",
      "Training Loss: 0.66232044\n",
      "Validation Loss: 1.2570508\n",
      "23\n",
      "Training Loss: 0.6140181\n",
      "Training Loss: 0.5799532\n",
      "Training Loss: 0.54016346\n",
      "Training Loss: 0.63334996\n",
      "Training Loss: 0.4991685\n",
      "Training Loss: 0.7625915\n",
      "Training Loss: 0.5899319\n",
      "Validation Loss: 1.3648496\n",
      "24\n",
      "Training Loss: 0.6396237\n",
      "Training Loss: 0.6233633\n",
      "Training Loss: 0.58223164\n",
      "Training Loss: 0.7505674\n",
      "Training Loss: 0.5478918\n",
      "Training Loss: 0.6922864\n",
      "Training Loss: 0.6733267\n",
      "Validation Loss: 1.1149544\n",
      "25\n",
      "Training Loss: 0.6093576\n",
      "Training Loss: 0.62280387\n",
      "Training Loss: 0.588529\n",
      "Training Loss: 0.5798622\n",
      "Training Loss: 0.5046313\n",
      "Training Loss: 0.67587423\n",
      "Training Loss: 0.6021892\n",
      "Validation Loss: 1.2965337\n",
      "26\n",
      "Training Loss: 0.6384884\n",
      "Training Loss: 0.589901\n",
      "Training Loss: 0.51393056\n",
      "Training Loss: 0.5361643\n",
      "Training Loss: 0.46989226\n",
      "Training Loss: 1.2334346\n",
      "Training Loss: 0.5688747\n",
      "Validation Loss: 2.320578\n",
      "27\n",
      "Training Loss: 0.76935637\n",
      "Training Loss: 0.99623746\n",
      "Training Loss: 1.2042679\n",
      "Training Loss: 1.457189\n",
      "Training Loss: 0.9786851\n",
      "Training Loss: 0.7345536\n",
      "Training Loss: 0.73980635\n",
      "Validation Loss: 1.0283017\n",
      "28\n",
      "Training Loss: 0.73348993\n",
      "Training Loss: 0.946612\n",
      "Training Loss: 0.9663515\n",
      "Training Loss: 0.56721157\n",
      "Training Loss: 0.6520844\n",
      "Training Loss: 1.0005983\n",
      "Training Loss: 0.7438056\n",
      "Validation Loss: 1.3730371\n",
      "29\n",
      "Training Loss: 0.64077675\n",
      "Training Loss: 0.7068004\n",
      "Training Loss: 0.813826\n",
      "Training Loss: 1.0499383\n",
      "Training Loss: 0.8402958\n",
      "Training Loss: 0.7313111\n",
      "Training Loss: 0.636642\n",
      "Validation Loss: 1.1212213\n"
     ]
    }
   ],
   "source": [
    "## Trainer\n",
    "model = EpiShift()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, \n",
    "                       weight_decay=5e-4)\n",
    "#scheduler = StepLR(optimizer, step_size=1, \n",
    "#                   gamma=0.5)\n",
    "min_val_loss = np.inf\n",
    "m = nn.Sigmoid()\n",
    "best_model = deepcopy(model)\n",
    "model.train()\n",
    "model.to(device)\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(30):\n",
    "    print(epoch)\n",
    "    model.train()\n",
    "    \n",
    "    # Training epoch\n",
    "    for step, data in enumerate(train_dataloader):\n",
    "        X = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(X.unsqueeze(1))\n",
    "        loss = criterion(m(pred).squeeze(), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss)\n",
    "        print(\"Training Loss:\", loss.detach().cpu().numpy())\n",
    "\n",
    "    # Validation step\n",
    "    val_loss_sum = 0\n",
    "    model.eval()\n",
    "    for step, data in enumerate(val_dataloader):\n",
    "        X = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "\n",
    "        val_pred = model(X.unsqueeze(1))\n",
    "        val_loss = criterion(m(val_pred).squeeze(), y)\n",
    "        val_loss_sum += val_loss\n",
    "        \n",
    "    print(\"Validation Loss:\", val_loss_sum.detach().cpu().numpy())\n",
    "    if val_loss_sum > min_val_loss:\n",
    "        min_val_loss = val_loss_sum\n",
    "        best_model = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "ys = []\n",
    "model.eval()\n",
    "\n",
    "for step, data in enumerate(test_dataloader):\n",
    "    X = data[0].to(device)\n",
    "    y = data[1].to(device)\n",
    "\n",
    "    test_pred = model(X.unsqueeze(1))\n",
    "    tp = test_pred.squeeze().cpu().detach().numpy()\n",
    "    test_preds.extend(tp)\n",
    "    ys.extend(y.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t.cpu().detach().numpy().flatten() for t in test_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(ys, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f89c3b41590>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0,1],[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EpiMap data download script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = 'wget https://epigenome.wustl.edu/epimap/data/observed/FINAL_DNase-seq_BSS'\n",
    "template2 = '.sub_VS_Uniform_BKG_CONTROL_36_50000000.pval.signal.bedgraph.gz.bigWig'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./EpiMap/dl_script2.txt', 'a') as out_file:\n",
    "    for number in range(10,999):\n",
    "        file_num = \"{:05d}\".format(number)\n",
    "        filename = template1 + file_num + template2\n",
    "        out_file.write(filename+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsnap",
   "language": "python",
   "name": "deepsnap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
